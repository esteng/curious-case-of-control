{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json \n",
    "\n",
    "from experiment import Experiment\n",
    "from api_tools import (FixedGPTPrompt, \n",
    "                       FixedPassiveGPTPrompt, \n",
    "                       FixedT5Prompt, \n",
    "                       FixedPassiveT5Prompt, \n",
    "                       run_ai21_prompt, \n",
    "                       run_ai21_jumbo_prompt, \n",
    "                       run_gpt_prompt, \n",
    "                       run_t5_prompt)\n",
    "\n",
    "from metrics import accuracy_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject Control\n",
    "Starting with the **subject** control experiments, we will look at how 4 LMs do:\n",
    "1. GPT3\n",
    "2. T5 for QA \n",
    "3. Jurassic large \n",
    "4. Jurassic Jumbo\n",
    "\n",
    "## Experimental settings\n",
    "We're looking at different name pairs to control for gender. We have 2 male-female name pairs, 1 male-male pair, 1 female-female pair, and 1 neutral-neutral. \n",
    "We're still looking at 5 different actions, but only one verb: promise. \n",
    "\n",
    "We can later coerce (with gender) subject control on \"suggested\" and \"proposed\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = [(\"Tom\", \"Mary\"), (\"Tom\", \"Bill\"), (\"Mary\", \"Bill\"), (\"Ellen\", \"Mary\"), (\"Morgan\", \"Jaime\"), (\"Iago\", \"Hamlet\"), (\"Jules\", \"Yves\"), (\"Kurt\", \"Lena\")]\n",
    "# names = [(\"Tom\", \"Mary\"), (\"Tom\", \"Bill\"), (\"Mary\", \"Bill\"), (\"Ellen\", \"Mary\"), (\"Morgan\", \"Jaime\")]\n",
    "names = json.load(open(\"../data/names_top_2.json\"))\n",
    "# verbs = [\"told\", \"ordered\", \"called upon\", \"reminded\", \"urged\", \"asked\", \"persuaded\", \"convinced\", \"forced\", \"pushed\"]\n",
    "verbs = [\"promised\"]\n",
    "# actions = [(\"to leave\", \"left\"), (\"to call home\", \"called home\"), (\"to reply\", \"replied\"), (\"to wipe the counter\", \"wiped the counter\"), (\"to dance\", \"danced\")]\n",
    "actions = json.load(open(\"../data/verbs.json\"))\n",
    "correct_index = 0\n",
    "\n",
    "nicknames = json.load(open(\"../data/nicknames.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 3\n",
    "For GPT3, inference is not deterministic, so we're running 5 replicants per prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [05:58<00:00, 11.94s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpt_kwargs = {\"max_tokens\": 2, \"temperature\": 0.0}\n",
    "gpt_subject_control_experiment  = Experiment(\"gpt3\", \"subject-control\", FixedGPTPrompt, run_gpt_prompt, 1, gpt_kwargs)\n",
    "\n",
    "gpt_subject_control_experiment.run(names, correct_index, verbs, actions, nicknames=nicknames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_df = gpt_subject_control_experiment.format_results()\n",
    "\n",
    "gpt_df.to_csv(\"../results/gpt_subject_control_swap_names.csv\")\n",
    "\n",
    "# gpt_df = pd.read_csv(\"/Users/Elias/child-lm/results/gpt_subject_control_swap_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': (0.48333333333333334, 300, 0.5471698113207547, 265),\n",
       " 'acc_by_name': {'Avery,Casey': (0.4, 20, 0.47058823529411764, 17),\n",
       "  'Avery,Nicole': (0.5, 20, 0.5, 20),\n",
       "  'Avery,Stephanie': (0.75, 20, 0.75, 20),\n",
       "  'Avery,William': (0.55, 20, 0.5789473684210527, 19),\n",
       "  'Avery,Joseph': (0.5, 20, 0.5263157894736842, 19),\n",
       "  'Casey,Nicole': (0.4, 20, 0.7272727272727273, 11),\n",
       "  'Casey,Stephanie': (0.35, 20, 0.4375, 16),\n",
       "  'Casey,William': (0.45, 20, 0.5625, 16),\n",
       "  'Casey,Joseph': (0.45, 20, 0.5294117647058824, 17),\n",
       "  'Nicole,Stephanie': (0.5, 20, 0.5, 20),\n",
       "  'Nicole,William': (0.5, 20, 0.5555555555555556, 18),\n",
       "  'Nicole,Joseph': (0.5, 20, 0.5, 20),\n",
       "  'Stephanie,William': (0.5, 20, 0.5, 20),\n",
       "  'Stephanie,Joseph': (0.3, 20, 0.375, 16),\n",
       "  'William,Joseph': (0.6, 20, 0.75, 16)},\n",
       " 'acc_by_action': {'to read': (0.45, 60, 0.5294117647058824, 51),\n",
       "  'to go': (0.5166666666666667, 60, 0.5535714285714286, 56),\n",
       "  'to come': (0.4666666666666667, 60, 0.5283018867924528, 53),\n",
       "  'to call': (0.4666666666666667, 60, 0.5833333333333334, 48),\n",
       "  'to run': (0.5166666666666667, 60, 0.543859649122807, 57)},\n",
       " 'acc_by_verb': {'promised': (0.48333333333333334,\n",
       "   300,\n",
       "   0.5471698113207547,\n",
       "   265)},\n",
       " 'acc_by_action_by_verb': {'to read,promised': (0.45,\n",
       "   60,\n",
       "   0.5294117647058824,\n",
       "   51),\n",
       "  'to go,promised': (0.5166666666666667, 60, 0.5535714285714286, 56),\n",
       "  'to come,promised': (0.4666666666666667, 60, 0.5283018867924528, 53),\n",
       "  'to call,promised': (0.4666666666666667, 60, 0.5833333333333334, 48),\n",
       "  'to run,promised': (0.5166666666666667, 60, 0.543859649122807, 57)}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_report(gpt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/estengel/child-lm/src/hf_tools/hf.py\u001b[0m(36)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     34 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     35 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 36 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0moutput_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     37 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     38 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "1\n",
      "1\n",
      "tensor([[   0, 6320,   63,   42,   71, 8461,   58,    1]])\n",
      "tensor([[   0, 6320,   63,   42,   71, 8461,   58,    1]])\n",
      "tensor([[   0, 6320,   63,   42,   71, 8461,   58,    1]])\n",
      "tensor([[   0, 6320,   63,   42,   71, 8461,   58,    1]])\n",
      "tensor([[   0, 6320,   63,   42,   71, 8461,   58,    1]])\n",
      "--KeyboardInterrupt--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:43<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt: Interrupted by user\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'output_text' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_93385/2682563035.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mt5_base_subject_control_experiment\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t5-base\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"subject-control\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFixedGPTPrompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper_fxn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mt5_base_subject_control_experiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_swap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnicknames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnicknames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate_limit_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mt5_base_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt5_base_subject_control_experiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/child-lm/src/experiment.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, names, correct_name_idx, verbs, actions, do_swap, qa_pair, overwrite, nicknames, rate_limit_delay)\u001b[0m\n\u001b[1;32m    108\u001b[0m                             \u001b[0;31m# # each replicant is already on a different line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                             \u001b[0;31m# replicants = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                         \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment_fxn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicants\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/child-lm/src/api_tools.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(run_fxn, text, replicants, metric, kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# don't do tqdm if only 1, it's annoying\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_fxn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m             \u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/child-lm/src/hf_tools/hf.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'output_text' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from hf_tools.hf import HuggingfaceRunFxn\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/brtx/601-nvme1/estengel/.cache\"\n",
    "\n",
    "wrapper_fxn = HuggingfaceRunFxn(\"t5-base\", device=\"cpu\", constrained=True) \n",
    "\n",
    "t5_base_subject_control_experiment  = Experiment(\"t5-base\", \"subject-control\", FixedGPTPrompt, wrapper_fxn, 1, None)\n",
    "\n",
    "t5_base_subject_control_experiment.run(names, correct_index, verbs, actions, do_swap = False, nicknames=nicknames, rate_limit_delay=None, overwrite=True)\n",
    "\n",
    "t5_base_df = t5_base_subject_control_experiment.format_results()\n",
    "\n",
    "t5_base_df.to_csv(\"../results/t5_base_subject_control.csv\")\n",
    "\n",
    "accuracy_report(t5_base_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cf8a2556184e819d1cd9c6602358a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b92a1af1b564cd7afa95b468d61f0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 30/30 [01:29<00:00,  2.98s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total': (0.86, 150, 0.8716216216216216, 148),\n",
       " 'acc_by_name': {'Avery,William': (0.8, 10, 0.8, 10),\n",
       "  'Avery,Casey': (0.6, 10, 0.6, 10),\n",
       "  'Avery,Nicole': (0.8, 10, 0.8, 10),\n",
       "  'Avery,Stephanie': (1.0, 10, 1.0, 10),\n",
       "  'Avery,Joseph': (0.4, 10, 0.4, 10),\n",
       "  'William,Casey': (1.0, 10, 1.0, 10),\n",
       "  'William,Nicole': (1.0, 10, 1.0, 10),\n",
       "  'William,Stephanie': (1.0, 10, 1.0, 10),\n",
       "  'William,Joseph': (1.0, 10, 1.0, 10),\n",
       "  'Casey,Nicole': (0.7, 10, 0.7, 10),\n",
       "  'Casey,Stephanie': (0.9, 10, 0.9, 10),\n",
       "  'Casey,Joseph': (0.8, 10, 0.8888888888888888, 9),\n",
       "  'Nicole,Stephanie': (1.0, 10, 1.0, 10),\n",
       "  'Nicole,Joseph': (0.9, 10, 1.0, 9),\n",
       "  'Stephanie,Joseph': (1.0, 10, 1.0, 10)},\n",
       " 'acc_by_action': {'to come': (0.8, 30, 0.8571428571428571, 28),\n",
       "  'to go': (0.8, 30, 0.8, 30),\n",
       "  'to run': (0.8666666666666667, 30, 0.8666666666666667, 30),\n",
       "  'to call': (0.9333333333333333, 30, 0.9333333333333333, 30),\n",
       "  'to read': (0.9, 30, 0.9, 30)},\n",
       " 'acc_by_verb': {'promised': (0.86, 150, 0.8716216216216216, 148)},\n",
       " 'acc_by_action_by_verb': {'to come,promised': (0.8,\n",
       "   30,\n",
       "   0.8571428571428571,\n",
       "   28),\n",
       "  'to go,promised': (0.8, 30, 0.8, 30),\n",
       "  'to run,promised': (0.8666666666666667, 30, 0.8666666666666667, 30),\n",
       "  'to call,promised': (0.9333333333333333, 30, 0.9333333333333333, 30),\n",
       "  'to read,promised': (0.9, 30, 0.9, 30)}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hf_tools.hf import HuggingfaceRunFxn\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/brtx/601-nvme1/estengel/.cache\"\n",
    "\n",
    "wrapper_fxn = HuggingfaceRunFxn(\"t5-large\", \"cuda:3\")\n",
    "\n",
    "t5_large_subject_control_experiment  = Experiment(\"t5-large\", \"subject-control\", FixedGPTPrompt, wrapper_fxn, 1, None)\n",
    "\n",
    "t5_large_subject_control_experiment.run(names, correct_index, verbs, actions, do_swap = False, nicknames=nicknames, rate_limit_delay=None, overwrite=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': (0.9933333333333333, 150, 0.9933333333333333, 150),\n",
       " 'acc_by_name': {'Avery,William': (1.0, 10, 1.0, 10),\n",
       "  'Avery,Casey': (1.0, 10, 1.0, 10),\n",
       "  'Avery,Nicole': (1.0, 10, 1.0, 10),\n",
       "  'Avery,Stephanie': (0.9, 10, 0.9, 10),\n",
       "  'Avery,Joseph': (1.0, 10, 1.0, 10),\n",
       "  'William,Casey': (1.0, 10, 1.0, 10),\n",
       "  'William,Nicole': (1.0, 10, 1.0, 10),\n",
       "  'William,Stephanie': (1.0, 10, 1.0, 10),\n",
       "  'William,Joseph': (1.0, 10, 1.0, 10),\n",
       "  'Casey,Nicole': (1.0, 10, 1.0, 10),\n",
       "  'Casey,Stephanie': (1.0, 10, 1.0, 10),\n",
       "  'Casey,Joseph': (1.0, 10, 1.0, 10),\n",
       "  'Nicole,Stephanie': (1.0, 10, 1.0, 10),\n",
       "  'Nicole,Joseph': (1.0, 10, 1.0, 10),\n",
       "  'Stephanie,Joseph': (1.0, 10, 1.0, 10)},\n",
       " 'acc_by_action': {'to come': (0.9666666666666667, 30, 0.9666666666666667, 30),\n",
       "  'to go': (1.0, 30, 1.0, 30),\n",
       "  'to run': (1.0, 30, 1.0, 30),\n",
       "  'to call': (1.0, 30, 1.0, 30),\n",
       "  'to read': (1.0, 30, 1.0, 30)},\n",
       " 'acc_by_verb': {'promised': (0.9933333333333333,\n",
       "   150,\n",
       "   0.9933333333333333,\n",
       "   150)},\n",
       " 'acc_by_action_by_verb': {'to come,promised': (0.9666666666666667,\n",
       "   30,\n",
       "   0.9666666666666667,\n",
       "   30),\n",
       "  'to go,promised': (1.0, 30, 1.0, 30),\n",
       "  'to run,promised': (1.0, 30, 1.0, 30),\n",
       "  'to call,promised': (1.0, 30, 1.0, 30),\n",
       "  'to read,promised': (1.0, 30, 1.0, 30)}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_large_df = t5_large_subject_control_experiment.format_results()\n",
    "\n",
    "t5_large_df.to_csv(\"../results/t5_large_subject_control.csv\")\n",
    "\n",
    "accuracy_report(t5_large_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 3B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-3b and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 30/30 [01:29<00:00,  2.98s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total': (0.28, 150, 1.0, 42),\n",
       " 'acc_by_name': {'Avery,William': (0.7, 10, 1.0, 7),\n",
       "  'Avery,Casey': (0.8, 10, 1.0, 8),\n",
       "  'Avery,Nicole': (0.0, 10, -1, 0),\n",
       "  'Avery,Stephanie': (0.3, 10, 1.0, 3),\n",
       "  'Avery,Joseph': (0.2, 10, 1.0, 2),\n",
       "  'William,Casey': (0.6, 10, 1.0, 6),\n",
       "  'William,Nicole': (0.0, 10, -1, 0),\n",
       "  'William,Stephanie': (0.3, 10, 1.0, 3),\n",
       "  'William,Joseph': (0.5, 10, 1.0, 5),\n",
       "  'Casey,Nicole': (0.3, 10, 1.0, 3),\n",
       "  'Casey,Stephanie': (0.0, 10, -1, 0),\n",
       "  'Casey,Joseph': (0.1, 10, 1.0, 1),\n",
       "  'Nicole,Stephanie': (0.2, 10, 1.0, 2),\n",
       "  'Nicole,Joseph': (0.0, 10, -1, 0),\n",
       "  'Stephanie,Joseph': (0.2, 10, 1.0, 2)},\n",
       " 'acc_by_action': {'to come': (0.36666666666666664, 30, 1.0, 11),\n",
       "  'to go': (0.3333333333333333, 30, 1.0, 10),\n",
       "  'to run': (0.2, 30, 1.0, 6),\n",
       "  'to call': (0.4, 30, 1.0, 12),\n",
       "  'to read': (0.1, 30, 1.0, 3)},\n",
       " 'acc_by_verb': {'promised': (0.28, 150, 1.0, 42)},\n",
       " 'acc_by_action_by_verb': {'to come,promised': (0.36666666666666664,\n",
       "   30,\n",
       "   1.0,\n",
       "   11),\n",
       "  'to go,promised': (0.3333333333333333, 30, 1.0, 10),\n",
       "  'to run,promised': (0.2, 30, 1.0, 6),\n",
       "  'to call,promised': (0.4, 30, 1.0, 12),\n",
       "  'to read,promised': (0.1, 30, 1.0, 3)}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hf_tools.hf import HuggingfaceRunFxn\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/brtx/601-nvme1/estengel/.cache\"\n",
    "\n",
    "wrapper_fxn = HuggingfaceRunFxn(\"t5-3b\", \"cuda:3\")\n",
    "\n",
    "t5_3b_subject_control_experiment  = Experiment(\"t5-3b\", \"subject-control\", FixedGPTPrompt, wrapper_fxn, 1, None)\n",
    "\n",
    "t5_3b_subject_control_experiment.run(names, correct_index, verbs, actions, do_swap = False, nicknames=nicknames, rate_limit_delay=None, overwrite=True)\n",
    "\n",
    "t5_3b_df = t5_3b_subject_control_experiment.format_results()\n",
    "\n",
    "t5_3b_df.to_csv(\"../results/t5_3b_subject_control.csv\")\n",
    "\n",
    "accuracy_report(t5_3b_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 for QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fbcb83b9fe437cadb6e2d0cf3436dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2539d655a7164cb895e64f80fdbc744b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8cb7db001d4121887d8d9ef075b14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47bdbd6b626746759c19b99e85975a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ade6da64bdb48889297737ffa0c524a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff07631ec76740b19afa30fb7c4e7bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:08<00:00,  3.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from hf_tools.hf import HuggingfaceRunFxn\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/brtx/601-nvme1/estengel/.cache\"\n",
    "\n",
    "wrapper_fxn = HuggingfaceRunFxn(\"valhalla/t5-base-qa-qg-hl\", \"cuda:3\")\n",
    "\n",
    "t5_subject_control_experiment  = Experiment(\"t5-qa\", \"subject-control\", FixedT5Prompt, wrapper_fxn, 1, None)\n",
    "\n",
    "t5_subject_control_experiment.run(names, correct_index, verbs, actions, do_swap = False, nicknames=nicknames, rate_limit_delay=None, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_df = t5_subject_control_experiment.format_results()\n",
    "\n",
    "t5_df.to_csv(\"../results/t5_qa_subject_control.csv\")\n",
    "\n",
    "# t5_df = pd.read_csv(\"/Users/Elias/child-lm/results/t5_subject_control.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': (0.006666666666666667, 150, 0.006666666666666667, 150),\n",
       " 'acc_by_name': {'Avery,William': (0.0, 10, 0.0, 10),\n",
       "  'Avery,Casey': (0.0, 10, 0.0, 10),\n",
       "  'Avery,Nicole': (0.0, 10, 0.0, 10),\n",
       "  'Avery,Stephanie': (0.0, 10, 0.0, 10),\n",
       "  'Avery,Joseph': (0.0, 10, 0.0, 10),\n",
       "  'William,Casey': (0.0, 10, 0.0, 10),\n",
       "  'William,Nicole': (0.0, 10, 0.0, 10),\n",
       "  'William,Stephanie': (0.0, 10, 0.0, 10),\n",
       "  'William,Joseph': (0.0, 10, 0.0, 10),\n",
       "  'Casey,Nicole': (0.0, 10, 0.0, 10),\n",
       "  'Casey,Stephanie': (0.0, 10, 0.0, 10),\n",
       "  'Casey,Joseph': (0.1, 10, 0.1, 10),\n",
       "  'Nicole,Stephanie': (0.0, 10, 0.0, 10),\n",
       "  'Nicole,Joseph': (0.0, 10, 0.0, 10),\n",
       "  'Stephanie,Joseph': (0.0, 10, 0.0, 10)},\n",
       " 'acc_by_action': {'to come': (0.0, 30, 0.0, 30),\n",
       "  'to go': (0.0, 30, 0.0, 30),\n",
       "  'to run': (0.0, 30, 0.0, 30),\n",
       "  'to call': (0.0, 30, 0.0, 30),\n",
       "  'to read': (0.03333333333333333, 30, 0.03333333333333333, 30)},\n",
       " 'acc_by_verb': {'promised': (0.006666666666666667,\n",
       "   150,\n",
       "   0.006666666666666667,\n",
       "   150)},\n",
       " 'acc_by_action_by_verb': {'to come,promised': (0.0, 30, 0.0, 30),\n",
       "  'to go,promised': (0.0, 30, 0.0, 30),\n",
       "  'to run,promised': (0.0, 30, 0.0, 30),\n",
       "  'to call,promised': (0.0, 30, 0.0, 30),\n",
       "  'to read,promised': (0.03333333333333333, 30, 0.03333333333333333, 30)}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_report(t5_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [57:55<00:00, 115.84s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total': (0.6733333333333333, 150, 0.6733333333333333, 150),\n",
       " 'acc_by_name': {'Casey,Joseph': (0.9, 10, 0.9, 10),\n",
       "  'Casey,Stephanie': (0.9, 10, 0.9, 10),\n",
       "  'Casey,William': (0.6, 10, 0.6, 10),\n",
       "  'Casey,Nicole': (1.0, 10, 1.0, 10),\n",
       "  'Casey,Avery': (0.6, 10, 0.6, 10),\n",
       "  'Joseph,Stephanie': (0.5, 10, 0.5, 10),\n",
       "  'Joseph,William': (0.6, 10, 0.6, 10),\n",
       "  'Joseph,Nicole': (0.9, 10, 0.9, 10),\n",
       "  'Joseph,Avery': (0.4, 10, 0.4, 10),\n",
       "  'Stephanie,William': (0.6, 10, 0.6, 10),\n",
       "  'Stephanie,Nicole': (0.7, 10, 0.7, 10),\n",
       "  'Stephanie,Avery': (0.6, 10, 0.6, 10),\n",
       "  'William,Nicole': (0.8, 10, 0.8, 10),\n",
       "  'William,Avery': (0.4, 10, 0.4, 10),\n",
       "  'Nicole,Avery': (0.6, 10, 0.6, 10)},\n",
       " 'acc_by_action': {'to read': (0.6, 30, 0.6, 30),\n",
       "  'to call': (0.9666666666666667, 30, 0.9666666666666667, 30),\n",
       "  'to run': (0.7, 30, 0.7, 30),\n",
       "  'to go': (0.5666666666666667, 30, 0.5666666666666667, 30),\n",
       "  'to come': (0.5333333333333333, 30, 0.5333333333333333, 30)},\n",
       " 'acc_by_verb': {'promised': (0.6733333333333333,\n",
       "   150,\n",
       "   0.6733333333333333,\n",
       "   150)},\n",
       " 'acc_by_action_by_verb': {'to read,promised': (0.6, 30, 0.6, 30),\n",
       "  'to call,promised': (0.9666666666666667, 30, 0.9666666666666667, 30),\n",
       "  'to run,promised': (0.7, 30, 0.7, 30),\n",
       "  'to go,promised': (0.5666666666666667, 30, 0.5666666666666667, 30),\n",
       "  'to come,promised': (0.5333333333333333, 30, 0.5333333333333333, 30)}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hf_tools.hf import HuggingfaceRunFxn\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/brtx/601-nvme1/estengel/.cache\"\n",
    "\n",
    "wrapper_fxn = HuggingfaceRunFxn(\"bigscience/T0pp\", device=\"cpu\", constrained=False)\n",
    "\n",
    "t0_subject_control_experiment  = Experiment(\"t0\", \"subject-control\", FixedGPTPrompt, wrapper_fxn, 1, None)\n",
    "\n",
    "t0_subject_control_experiment.run(names, correct_index, verbs, actions, do_swap = False, nicknames=nicknames, rate_limit_delay=None, overwrite=True)\n",
    "\n",
    "t0_df = t0_subject_control_experiment.format_results()\n",
    "\n",
    "t0_df.to_csv(\"../results/t0_subject_control.csv\")\n",
    "\n",
    "accuracy_report(t0_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  3%|▎         | 1/30 [04:37<2:14:18, 277.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  7%|▋         | 2/30 [09:23<2:11:41, 282.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 10%|█         | 3/30 [13:33<2:00:26, 267.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 13%|█▎        | 4/30 [17:30<1:50:48, 255.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|█▋        | 5/30 [22:14<1:50:45, 265.84s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 20%|██        | 6/30 [26:25<1:44:16, 260.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 23%|██▎       | 7/30 [31:00<1:41:42, 265.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 27%|██▋       | 8/30 [35:14<1:35:59, 261.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 30%|███       | 9/30 [39:41<1:32:12, 263.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|███▎      | 10/30 [45:02<1:33:42, 281.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 37%|███▋      | 11/30 [50:42<1:34:46, 299.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 40%|████      | 12/30 [55:56<1:31:08, 303.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 43%|████▎     | 13/30 [1:01:40<1:29:31, 315.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 47%|████▋     | 14/30 [1:06:26<1:21:48, 306.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|█████     | 15/30 [1:11:56<1:18:25, 313.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 53%|█████▎    | 16/30 [1:16:24<1:10:02, 300.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 57%|█████▋    | 17/30 [1:20:46<1:02:31, 288.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 60%|██████    | 18/30 [1:25:53<58:48, 294.04s/it]  Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 63%|██████▎   | 19/30 [1:30:03<51:31, 281.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|██████▋   | 20/30 [1:35:38<49:31, 297.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 70%|███████   | 21/30 [1:40:04<43:09, 287.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 73%|███████▎  | 22/30 [1:44:05<36:30, 273.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 77%|███████▋  | 23/30 [1:48:52<32:23, 277.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 80%|████████  | 24/30 [1:53:22<27:33, 275.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|████████▎ | 25/30 [1:58:11<23:17, 279.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 87%|████████▋ | 26/30 [2:03:33<19:28, 292.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 90%|█████████ | 27/30 [2:09:14<15:20, 306.91s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 93%|█████████▎| 28/30 [2:15:40<11:00, 330.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 97%|█████████▋| 29/30 [2:21:25<05:34, 334.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 30/30 [2:26:33<00:00, 293.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total': (1.0, 150, 1.0, 150),\n",
       " 'acc_by_name': {'Casey,William': (1.0, 10, 1.0, 10),\n",
       "  'Casey,Stephanie': (1.0, 10, 1.0, 10),\n",
       "  'Casey,Joseph': (1.0, 10, 1.0, 10),\n",
       "  'Casey,Avery': (1.0, 10, 1.0, 10),\n",
       "  'Casey,Nicole': (1.0, 10, 1.0, 10),\n",
       "  'William,Stephanie': (1.0, 10, 1.0, 10),\n",
       "  'William,Joseph': (1.0, 10, 1.0, 10),\n",
       "  'William,Avery': (1.0, 10, 1.0, 10),\n",
       "  'William,Nicole': (1.0, 10, 1.0, 10),\n",
       "  'Stephanie,Joseph': (1.0, 10, 1.0, 10),\n",
       "  'Stephanie,Avery': (1.0, 10, 1.0, 10),\n",
       "  'Stephanie,Nicole': (1.0, 10, 1.0, 10),\n",
       "  'Nicole,Joseph': (1.0, 10, 1.0, 10),\n",
       "  'Nicole,Avery': (1.0, 10, 1.0, 10),\n",
       "  'Joseph,Avery': (1.0, 10, 1.0, 10)},\n",
       " 'acc_by_action': {'to read': (1.0, 30, 1.0, 30),\n",
       "  'to come': (1.0, 30, 1.0, 30),\n",
       "  'to run': (1.0, 30, 1.0, 30),\n",
       "  'to go': (1.0, 30, 1.0, 30),\n",
       "  'to call': (1.0, 30, 1.0, 30)},\n",
       " 'acc_by_verb': {'promised': (1.0, 150, 1.0, 150)},\n",
       " 'acc_by_action_by_verb': {'to read,promised': (1.0, 30, 1.0, 30),\n",
       "  'to come,promised': (1.0, 30, 1.0, 30),\n",
       "  'to run,promised': (1.0, 30, 1.0, 30),\n",
       "  'to go,promised': (1.0, 30, 1.0, 30),\n",
       "  'to call,promised': (1.0, 30, 1.0, 30)}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hf_tools.hf import HuggingfaceRunFxn\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/brtx/601-nvme1/estengel/.cache\"\n",
    "\n",
    "wrapper_fxn = HuggingfaceRunFxn(\"EleutherAI/gpt-neo-2.7B\", device=\"cpu\", constrained=False)\n",
    "\n",
    "gptneo_subject_control_experiment  = Experiment(\"gpt-neo-2.7b\", \"subject-control\", FixedGPTPrompt, wrapper_fxn, 1, None)\n",
    "\n",
    "gptneo_subject_control_experiment.run(names, correct_index, verbs, actions, do_swap = False, nicknames=nicknames, rate_limit_delay=None, overwrite=True)\n",
    "\n",
    "gptneo_df = gptneo_subject_control_experiment.format_results()\n",
    "\n",
    "gptneo_df.to_csv(\"../results/gpt_neo_2.7b_subject_control.csv\")\n",
    "\n",
    "accuracy_report(gptneo_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jurassic Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [05:58<00:00, 11.93s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "jurassic_kwargs = {\"maxTokens\": 2, \"temperature\": 0.0}\n",
    "jurassic_subject_control_experiment  = Experiment(\"jurassic-large\", \"subject-control\", FixedGPTPrompt, run_ai21_prompt, 1, jurassic_kwargs)\n",
    "\n",
    "jurassic_subject_control_experiment.run(names, correct_index, verbs, actions, nicknames=nicknames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "jurassic_df = jurassic_subject_control_experiment.format_results()\n",
    "\n",
    "jurassic_df.to_csv(\"../results/jurassic_subject_control_swap_names.csv\")\n",
    "\n",
    "# jurassic_df = pd.read_csv(\"/Users/Elias/child-lm/results/jurassic_subject_control_swap_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': (0.63, 300, 0.6385135135135135, 296),\n",
       " 'acc_by_name': {'Avery,Casey': (0.6, 20, 0.6, 20),\n",
       "  'Avery,Nicole': (0.65, 20, 0.7222222222222222, 18),\n",
       "  'Avery,Stephanie': (0.75, 20, 0.75, 20),\n",
       "  'Avery,William': (0.65, 20, 0.65, 20),\n",
       "  'Avery,Joseph': (0.7, 20, 0.7, 20),\n",
       "  'Casey,Nicole': (0.75, 20, 0.75, 20),\n",
       "  'Casey,Stephanie': (0.55, 20, 0.55, 20),\n",
       "  'Casey,William': (0.5, 20, 0.5, 20),\n",
       "  'Casey,Joseph': (0.55, 20, 0.55, 20),\n",
       "  'Nicole,Stephanie': (0.45, 20, 0.5, 18),\n",
       "  'Nicole,William': (0.6, 20, 0.6, 20),\n",
       "  'Nicole,Joseph': (0.7, 20, 0.7, 20),\n",
       "  'Stephanie,William': (0.5, 20, 0.5, 20),\n",
       "  'Stephanie,Joseph': (0.75, 20, 0.75, 20),\n",
       "  'William,Joseph': (0.75, 20, 0.75, 20)},\n",
       " 'acc_by_action': {'to read': (0.6333333333333333, 60, 0.6333333333333333, 60),\n",
       "  'to go': (0.6166666666666667, 60, 0.6271186440677966, 59),\n",
       "  'to come': (0.65, 60, 0.65, 60),\n",
       "  'to call': (0.5666666666666667, 60, 0.5964912280701754, 57),\n",
       "  'to run': (0.6833333333333333, 60, 0.6833333333333333, 60)},\n",
       " 'acc_by_verb': {'promised': (0.63, 300, 0.6385135135135135, 296)},\n",
       " 'acc_by_action_by_verb': {'to read,promised': (0.6333333333333333,\n",
       "   60,\n",
       "   0.6333333333333333,\n",
       "   60),\n",
       "  'to go,promised': (0.6166666666666667, 60, 0.6271186440677966, 59),\n",
       "  'to come,promised': (0.65, 60, 0.65, 60),\n",
       "  'to call,promised': (0.5666666666666667, 60, 0.5964912280701754, 57),\n",
       "  'to run,promised': (0.6833333333333333, 60, 0.6833333333333333, 60)}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_report(jurassic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jurassic Jumbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [17:30<00:00, 35.00s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "jurassic_kwargs = {\"maxTokens\": 2, \"temperature\": 0.0}\n",
    "jurassic_jumbo_subject_control_experiment  = Experiment(\"jurassic-jumbo\", \"subject-control\", FixedGPTPrompt, run_ai21_jumbo_prompt, 1, jurassic_kwargs)\n",
    "\n",
    "jurassic_jumbo_subject_control_experiment.run(names, correct_index, verbs, actions, nicknames=nicknames, do_swap=True, rate_limit_delay=60, rate_limit_count=19)\n",
    "jurassic_jumbo_df = jurassic_jumbo_subject_control_experiment.format_results()\n",
    "\n",
    "jurassic_jumbo_df.to_csv(\"../results/jurassic_jumbo_subject_control_swap_names.csv\")\n",
    "\n",
    "# jurassic_df = pd.read_csv(\"/Users/Elias/child-lm/results/jurassic_subject_control_swap_names.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Coerced examples with gender\n",
    "\n",
    "By using gendered names and pronouns, we can coerce subject or object control from \"suggested\", \"offered\", and \"proposed\", e.g. \n",
    "\n",
    "- Mary proposed to Tom to be his editor\n",
    "- Tom suggested to Mary to be her editor \n",
    "- Mary offered to Tom to be his editor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = [\"promised\", \"offered\", \"suggested\", \"proposed\"]\n",
    "his_names = [(\"Tom\", \"Mary\"), (\"Bill\", \"Mary\"), (\"James\", \"Mary\"), (\"Tom\", \"Sally\"), (\"Bill\", \"Sally\"), (\"James\", \"Sally\")]\n",
    "actions = [(\"to be her editor\", \"was the editor\")]\n",
    "correct_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gpt_kwargs = {\"max_tokens\": 2, \"temperature\": 0.0}\n",
    "# gendered_gpt_subject_control_experiment  = Experiment(\"gpt3\", \"subject-control\", FixedGPTPrompt, run_gpt_prompt, 5, gpt_kwargs)\n",
    "# gendered_gpt_subject_control_experiment.run(his_names, correct_index, verbs, actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gendered_gpt_df = gendered_gpt_subject_control_experiment.format_results()\n",
    "\n",
    "# gendered_gpt_df.to_csv(\"/Users/Elias/child-lm/results/gpt_gendered_subject_control_swap_names.csv\")\n",
    "gendered_gpt_df = pd.read_csv(\"/Users/Elias/child-lm/results/gpt_gendered_subject_control_swap_names.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': (0.3125, 48),\n",
       " 'acc_by_name': {'James,Sally': (0.5, 8),\n",
       "  'James,Mary': (0.0, 8),\n",
       "  'Tom,Sally': (0.625, 8),\n",
       "  'Tom,Mary': (0.125, 8),\n",
       "  'Bill,Sally': (0.625, 8),\n",
       "  'Bill,Mary': (0.0, 8)},\n",
       " 'acc_by_action': {'to be her editor': (0.3125, 48)},\n",
       " 'acc_by_verb': {'promised': (0.4166666666666667, 12),\n",
       "  'suggested': (0.25, 12),\n",
       "  'proposed': (0.3333333333333333, 12),\n",
       "  'offered': (0.25, 12)},\n",
       " 'acc_by_action_by_verb': {'to be her editor,promised': (0.4166666666666667,\n",
       "   12),\n",
       "  'to be her editor,suggested': (0.25, 12),\n",
       "  'to be her editor,proposed': (0.3333333333333333, 12),\n",
       "  'to be her editor,offered': (0.25, 12)}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_report(gendered_gpt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 for QA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gendered_t5_subject_control_experiment  = Experiment(\"t5\", \"subject-control\", FixedT5Prompt, run_t5_prompt, 1, None)\n",
    "# gendered_t5_subject_control_experiment.run(his_names, correct_index, verbs, actions)\n",
    "# gendered_t5_df = gendered_t5_subject_control_experiment.format_results()\n",
    "# gendered_t5_df.to_csv(\"/Users/Elias/child-lm/results/t5_gendered_subject_control_swap_names.csv\")\n",
    "gendered_t5_df = pd.read_csv(\"/Users/Elias/child-lm/results/t5_gendered_subject_control_swap_names.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': (0.0, 48),\n",
       " 'acc_by_name': {'James,Sally': (0.0, 8),\n",
       "  'James,Mary': (0.0, 8),\n",
       "  'Tom,Sally': (0.0, 8),\n",
       "  'Tom,Mary': (0.0, 8),\n",
       "  'Bill,Sally': (0.0, 8),\n",
       "  'Bill,Mary': (0.0, 8)},\n",
       " 'acc_by_action': {'to be her editor': (0.0, 48)},\n",
       " 'acc_by_verb': {'promised': (0.0, 12),\n",
       "  'suggested': (0.0, 12),\n",
       "  'proposed': (0.0, 12),\n",
       "  'offered': (0.0, 12)},\n",
       " 'acc_by_action_by_verb': {'to be her editor,promised': (0.0, 12),\n",
       "  'to be her editor,suggested': (0.0, 12),\n",
       "  'to be her editor,proposed': (0.0, 12),\n",
       "  'to be her editor,offered': (0.0, 12)}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_report(gendered_t5_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jurassic Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jurassic_kwargs = {\"maxTokens\": 2, \"temperature\": 0.0}\n",
    "# gendered_jurassic_subject_control_experiment  = Experiment(\"jurassic\", \"subject-control\", FixedGPTPrompt, run_ai21_prompt, 1, jurassic_kwargs)\n",
    "# gendered_jurassic_subject_control_experiment.run(his_names, correct_index, verbs, actions)\n",
    "# gendered_jurassic_df = gendered_jurassic_subject_control_experiment.format_results()\n",
    "# gendered_jurassic_df.to_csv(\"/Users/Elias/child-lm/results/jurassic_gendered_subject_control_swap_names.csv\")\n",
    "gendered_jurassic_df = pd.read_csv(\"/Users/Elias/child-lm/results/jurassic_gendered_subject_control_swap_names.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': (0.6875, 48),\n",
       " 'acc_by_name': {'James,Sally': (1.0, 8),\n",
       "  'James,Mary': (0.625, 8),\n",
       "  'Tom,Sally': (0.875, 8),\n",
       "  'Tom,Mary': (0.625, 8),\n",
       "  'Bill,Sally': (0.5, 8),\n",
       "  'Bill,Mary': (0.5, 8)},\n",
       " 'acc_by_action': {'to be her editor': (0.6875, 48)},\n",
       " 'acc_by_verb': {'promised': (0.8333333333333334, 12),\n",
       "  'suggested': (0.5833333333333334, 12),\n",
       "  'proposed': (0.6666666666666666, 12),\n",
       "  'offered': (0.6666666666666666, 12)},\n",
       " 'acc_by_action_by_verb': {'to be her editor,promised': (0.8333333333333334,\n",
       "   12),\n",
       "  'to be her editor,suggested': (0.5833333333333334, 12),\n",
       "  'to be her editor,proposed': (0.6666666666666666, 12),\n",
       "  'to be her editor,offered': (0.6666666666666666, 12)}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "accuracy_report(gendered_jurassic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passives \n",
    "\n",
    "Do passives here make sense? To me \n",
    "- Mary was promised by Tom to leave\n",
    "Does not sound acceptable, or if it is accepetable, Mary is the one leaving, unlike \"Tom promised Mary to leave\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec6008f546f3cd66431ed7a4f2e2b63949ea9a2fd324a00b9368fd8a5a5333f5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('openai': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
