{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json \n",
    "\n",
    "import os \n",
    "import sys\n",
    "\n",
    "import pdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = json.load(open(\"../../data/names_top_2.json\"))\n",
    "verbs = [\"promised\"]\n",
    "actions = json.load(open(\"../../data/verbs.json\"))\n",
    "correct_index = 0\n",
    "\n",
    "nicknames = json.load(open(\"../../data/nicknames.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "import re \n",
    "\n",
    "class HuggingfaceRunFxn:\n",
    "    def __init__(self, model_name, constrained = False, device=\"cpu\"): \n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.padding_side = \"left\" \n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "        self.device = device \n",
    "        self.model.to(device)\n",
    "        print(self.model.config.__dict__)\n",
    "        self.model.config.return_dict_in_generate=True\n",
    "        self.constrained = constrained\n",
    "\n",
    "    def get_names(self, text): \n",
    "        # get names from the prompt \n",
    "        n1, n2 = re.findall(\"Answer the question with either \\\"(\\w+)\\\" or \\\"(\\w+)\\\"\", text)[0]\n",
    "        return n1, n2\n",
    "\n",
    "    def __call__(self, text, kwargs):\n",
    "        input_ids = self.tokenizer.encode(text, return_tensors='pt')\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        outputs =  self.model.generate(input_ids, \n",
    "                                       output_scores = True, \n",
    "                                       return_dict_in_generate=True, \n",
    "                                       num_beams=1, \n",
    "                                       max_length=100)\n",
    "        print(outputs)\n",
    "        if not self.constrained:\n",
    "            output_text = self.tokenizer.decode(outputs[0].to(\"cpu\"), skip_special_tokens=True)\n",
    "        else:\n",
    "            n1, n2 = self.get_names(text)\n",
    "            output_text = self.tokenizer.decode(outputs[0].to(\"cpu\"), skip_special_tokens=True)\n",
    "            # print(type(outputs))\n",
    "            # print(outputs)\n",
    "\n",
    "        return output_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043e184eaf12464dbfd342fce09fc10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/779k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e632f81f7d6748d6be87214e782e403d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca40679d32d44b1e8dc497fd44a0e692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69fd002697b43cdbe5af62e07573f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48cb7d5e6854d459a20d5c5ec650803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5666fdaea854ea09a8b4c51327d3438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for 'EleutherAI/gpt-neo-1.3B' at '/brtx/601-nvme1/estengel/.cache/7c5fac9d60b015cbc7c007ab8fe6d0512787fbaef81968922959898c49468d73.4c6a483fbfb5a25ac384bfcd71a1ff15245f06583a00c4ab4c44ed0f761f0b08'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/openai/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1359\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m                     \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/openai/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/openai/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: version_ <= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED at /pytorch/caffe2/serialize/inline_container.cc:132, please report a bug to PyTorch. Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2. Your PyTorch installation may be too old. (init at /pytorch/caffe2/serialize/inline_container.cc:132)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x33 (0x7f9b080b8193 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: caffe2::serialize::PyTorchStreamReader::init() + 0x1f5b (0x7f9b0b2409eb in /brtx/601-nvme1/estengel/miniconda3/envs/openai/lib/python3.7/site-packages/torch/lib/libtorch.so)\nframe #2: caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader(std::string const&) + 0x64 (0x7f9b0b241c04 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/lib/python3.7/site-packages/torch/lib/libtorch.so)\nframe #3: <unknown function> + 0x6c6536 (0x7f9b53172536 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #4: <unknown function> + 0x295a74 (0x7f9b52d41a74 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #5: _PyMethodDef_RawFastCallDict + 0x24c (0x55773170f71c in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #6: _PyObject_FastCallDict + 0x6e (0x5577316dff5e in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #7: <unknown function> + 0x12f041 (0x5577316f5041 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #8: PyObject_Call + 0x66 (0x5577316e07b6 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #9: <unknown function> + 0xc239e (0x55773168839e in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #10: _PyObject_FastCallKeywords + 0x54c (0x557731745c7c in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #11: <unknown function> + 0x1802d1 (0x5577317462d1 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #12: _PyEval_EvalFrameDefault + 0x48a2 (0x55773178d602 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #13: _PyEval_EvalCodeWithName + 0x79e (0x5577316df0de in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #14: _PyObject_FastCallDict + 0x312 (0x5577316e0202 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #15: <unknown function> + 0x186bef (0x55773174cbef in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #16: _PyObject_FastCallKeywords + 0x54c (0x557731745c7c in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #17: _PyEval_EvalFrameDefault + 0x47e5 (0x55773178d545 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #18: _PyEval_EvalCodeWithName + 0x273 (0x5577316debb3 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #19: _PyFunction_FastCallKeywords + 0x693 (0x5577316ff223 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #20: <unknown function> + 0x1800c5 (0x5577317460c5 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #21: _PyEval_EvalFrameDefault + 0x145c (0x55773178a1bc in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #22: _PyEval_EvalCodeWithName + 0x273 (0x5577316debb3 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #23: _PyFunction_FastCallKeywords + 0x693 (0x5577316ff223 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #24: <unknown function> + 0x1800c5 (0x5577317460c5 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #25: _PyEval_EvalFrameDefault + 0x48a2 (0x55773178d602 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #26: _PyEval_EvalCodeWithName + 0x273 (0x5577316debb3 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #27: _PyObject_FastCallDict + 0x5be (0x5577316e04ae in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #28: <unknown function> + 0x186bef (0x55773174cbef in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #29: _PyObject_FastCallKeywords + 0x54c (0x557731745c7c in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #30: <unknown function> + 0x1802d1 (0x5577317462d1 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #31: _PyEval_EvalFrameDefault + 0x145c (0x55773178a1bc in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #32: _PyEval_EvalCodeWithName + 0x273 (0x5577316debb3 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #33: <unknown function> + 0x1d751e (0x55773179d51e in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #34: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x55773170f959 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #35: _PyEval_EvalFrameDefault + 0x44f8 (0x55773178d258 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #36: <unknown function> + 0x17f0b4 (0x5577317450b4 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #37: _PyEval_EvalFrameDefault + 0x19f1 (0x55773178a751 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #38: <unknown function> + 0x17f0b4 (0x5577317450b4 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #39: _PyEval_EvalFrameDefault + 0x19f1 (0x55773178a751 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #40: <unknown function> + 0x17f0b4 (0x5577317450b4 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #41: _PyMethodDescr_FastCallKeywords + 0xdb (0x55773174546b in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #42: <unknown function> + 0x1801ae (0x5577317461ae in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #43: _PyEval_EvalFrameDefault + 0x621 (0x557731789381 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #44: _PyFunction_FastCallKeywords + 0x187 (0x5577316fed17 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #45: _PyEval_EvalFrameDefault + 0x3f5 (0x557731789155 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #46: _PyFunction_FastCallKeywords + 0x187 (0x5577316fed17 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #47: <unknown function> + 0x1800c5 (0x5577317460c5 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #48: _PyEval_EvalFrameDefault + 0x621 (0x557731789381 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #49: _PyEval_EvalCodeWithName + 0x273 (0x5577316debb3 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #50: _PyObject_FastCallDict + 0x5be (0x5577316e04ae in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #51: <unknown function> + 0x12f041 (0x5577316f5041 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #52: PyObject_Call + 0x66 (0x5577316e07b6 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #53: _PyEval_EvalFrameDefault + 0x1d0d (0x55773178aa6d in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #54: _PyEval_EvalCodeWithName + 0x79e (0x5577316df0de in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #55: _PyFunction_FastCallKeywords + 0x693 (0x5577316ff223 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #56: <unknown function> + 0x1800c5 (0x5577317460c5 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #57: _PyEval_EvalFrameDefault + 0x145c (0x55773178a1bc in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #58: <unknown function> + 0x17f0b4 (0x5577317450b4 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #59: _PyEval_EvalFrameDefault + 0x19f1 (0x55773178a751 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #60: <unknown function> + 0x17f0b4 (0x5577317450b4 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #61: _PyEval_EvalFrameDefault + 0x19f1 (0x55773178a751 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #62: <unknown function> + 0x17f0b4 (0x5577317450b4 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\nframe #63: _PyEval_EvalFrameDefault + 0x19f1 (0x55773178a751 in /brtx/601-nvme1/estengel/miniconda3/envs/openai/bin/python)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/openai/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m                             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"version\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m                                 raise OSError(\n",
      "\u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/openai/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_107894/653205052.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mfxn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuggingfaceRunFxn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EleutherAI/gpt-neo-1.3B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfxn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_107894/3586457674.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name, constrained, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPTNeoForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/openai/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1372\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                         raise OSError(\n\u001b[0;32m-> 1374\u001b[0;31m                             \u001b[0;34mf\"Unable to load weights from pytorch checkpoint file for '{pretrained_model_name_or_path}' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m                             \u001b[0;34mf\"at '{resolved_archive_file}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m                             \u001b[0;34m\"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for 'EleutherAI/gpt-neo-1.3B' at '/brtx/601-nvme1/estengel/.cache/7c5fac9d60b015cbc7c007ab8fe6d0512787fbaef81968922959898c49468d73.4c6a483fbfb5a25ac384bfcd71a1ff15245f06583a00c4ab4c44ed0f761f0b08'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/brtx/601-nvme1/estengel/.cache\"\n",
    "\n",
    "\n",
    "\n",
    "prompt = \"\"\"You will be given a context and a question. Answer the question with either \"Avery\" or \"Casey\".\n",
    "Context: Avery told Casey to come.\n",
    "\n",
    "Question:  Who came, Avery or Casey?\n",
    "Answer: \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "fxn = HuggingfaceRunFxn(\"EleutherAI/gpt-neo-1.3B\", constrained=True, device=\"cpu\")\n",
    "\n",
    "res = fxn(prompt, None)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3121ba8541e67f7e839f305684ea3814f59c71974b3238c74aa3730a425f63bc"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('openai': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
