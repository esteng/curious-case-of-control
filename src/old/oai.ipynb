{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api_tools import GPTPrompt, run_experiment, run_gpt_prompt, StringMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "Children up to the age of 8 incorrectly coindex PRO with the subject in cases of object control, such as \"x is easy to see\".\n",
    "\n",
    "## Does GPT-3 do this as well? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:06<00:08,  1.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d2/d0n6tsxs5cq1hbszm2w1sw1m0000gn/T/ipykernel_4963/537320359.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext1a\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m\"\"\"Answer the following question with either \"Easy to see\" or \"Hard to see\".\\nQuestion: Is a person with a blindfold on easy to see, or hard to see?\\nAnswer: \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"max_tokens\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"temperature\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_gpt_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext1a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicants\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"PROMPT:\\n{text1a}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/child-lm/src/api_tools.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(run_fxn, text, replicants, metric, kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_fxn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/child-lm/src/api_tools.py\u001b[0m in \u001b[0;36mrun_gpt_prompt\u001b[0;34m(text, kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \"\"\"\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_chunk_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\";\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/openai/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/openai/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/openai/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "replicants = 20\n",
    "class_lookups = {\"easy\": [\"easy\"], \"hard\": [\"hard\", \"difficult\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text1a =  \"\"\"Answer the following question with either \"Easy to see\" or \"Hard to see\".\\nQuestion: Is a person with a blindfold on easy to see, or hard to see?\\nAnswer: \"\"\"\n",
    "kwargs = {\"max_tokens\": 8, \"temperature\": 0.1}\n",
    "metric, responses = run_experiment(run_gpt_prompt, text1a, replicants, metric, kwargs)\n",
    "\n",
    "print(f\"PROMPT:\\n{text1a}\")\n",
    "# print(f\"RESPONSE:\\n-----------{resp}\")\n",
    "counts, classes = metric.get_metric()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'easy': 20, 'hard': 0, 'other': 0}\n"
     ]
    }
   ],
   "source": [
    "print(counts)\n",
    "\n",
    "# for c, resps in classes.items():\n",
    "#    print(c)\n",
    "#    for r in resps:\n",
    "#        r = r.split(\"\\n\")[0]\n",
    "#        print(f\"\\t{r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it just picking the second option? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:13<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "Answer the following question with either \"Easy to see\" or \"Hard to see\".\n",
      "Question: Is a person with a blindfold on hard to see, or easy to see?\n",
      "Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "replicants = 20\n",
    "class_lookups = {\"easy\": [\"easy\"], \"hard\": [\"hard\", \"difficult\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text1a =  \"\"\"Answer the following question with either \"Easy to see\" or \"Hard to see\".\\nQuestion: Is a person with a blindfold on hard to see, or easy to see?\\nAnswer: \"\"\"\n",
    "kwargs = {\"max_tokens\": 8, \"temperature\": 0.1}\n",
    "metric, responses = run_experiment(run_gpt_prompt, text1a, replicants, metric, kwargs)\n",
    "\n",
    "print(f\"PROMPT:\\n{text1a}\")\n",
    "counts, classes = metric.get_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'easy': 20, 'hard': 0, 'other': 0}\n"
     ]
    }
   ],
   "source": [
    "print(counts)\n",
    "\n",
    "# for c, resps in classes.items():\n",
    "#    print(c)\n",
    "#    for r in resps:\n",
    "#        r = r.split(\"\\n\")[0]\n",
    "#        print(f\"\\t{r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if you prompt with other correct examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'easy': 0, 'hard': 20, 'other': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = [\"\"\"Answer the following question with either \"Easy to see\" or \"Hard to see\".\\n\"\"\",\n",
    "           \"Question: Is a child with a blindfold on easy to see, or hard to see?\", \"A: \\\" Easy to see . \\\"\",\n",
    "           \"Question: Is a doll with a blindfold on easy to see, or hard to see?\", \"A: \\\" Easy to see . \\\"\",\n",
    "           \"Question: Is a person with a blindfold on easy to see, or hard to see?\"]\n",
    "prompt_text = \"Answer: \"\n",
    "\n",
    "prompt = GPTPrompt(context, prompt_text)\n",
    "\n",
    "replicants = 20\n",
    "class_lookups = {\"easy\": [\"easy\"], \"hard\": [\"hard\", \"difficult\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text = str(prompt)\n",
    "kwargs = {\"max_tokens\": 8, \"temperature\": 0.1}\n",
    "metric, responses = run_experiment(run_gpt_prompt, text, replicants, metric, kwargs)\n",
    "counts, classes = metric.get_metric()\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if you prompt with incorrect examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:13<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'easy': 20, 'hard': 0, 'other': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = [\"\"\"Answer the following question with either \"Easy to see\" or \"Hard to see\".\\n\"\"\",\n",
    "           \"Question: Is a child with a blindfold on easy to see, or hard to see?\", \"A: \\\" Hard to see . \\\"\",\n",
    "           \"Question: Is a doll with a blindfold on easy to see, or hard to see?\", \"A: \\\" Hard to see . \\\"\",\n",
    "           \"Question: Is a person with a blindfold on easy to see, or hard to see?\"]\n",
    "prompt_text = \"Answer: \"\n",
    "\n",
    "prompt = GPTPrompt(context, prompt_text)\n",
    "\n",
    "replicants = 20\n",
    "class_lookups = {\"easy\": [\"easy\"], \"hard\": [\"hard\", \"difficult\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text = str(prompt)\n",
    "kwargs = {\"max_tokens\": 8, \"temperature\": 0.1}\n",
    "metric, responses = run_experiment(run_gpt_prompt, text, replicants, metric, kwargs)\n",
    "counts, classes = metric.get_metric()\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions so far \n",
    "1. Chooses \"easy to see\" always when unprompted\n",
    "2. If prompted, it just chooses the opposite of what it's prompted with\n",
    "    - if it's prompted with \"easy to see\" it always chooses \"hard to see\" and vice versa, so prompting isn't as interesting \n",
    "    - probably picking up on some heuristic from tests that the easy answer isn't the right one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does wording matter? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:13<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No priming: {'easy': 0, 'hard': 0, 'other': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:12<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With priming: {'easy': 0, 'hard': 20, 'other': 0}\n"
     ]
    }
   ],
   "source": [
    "context = [\"\"\"Answer the following question with either \"Easy to see\" or \"Hard to see\".\\n\"\"\",\n",
    "           \"Question: Is a person wearing a blindfold easy to see, or hard to see?\"]\n",
    "prompt_text = \"Answer: \"\n",
    "\n",
    "prompt = GPTPrompt(context, prompt_text)\n",
    "replicants = 20\n",
    "class_lookups = {\"easy\": [\"easy\"], \"hard\": [\"hard\", \"difficult\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text = str(prompt)\n",
    "kwargs = {\"max_tokens\": 8, \"temperature\": 0.1}\n",
    "metric, responses = run_experiment(run_gpt_prompt, text, replicants, metric, kwargs)\n",
    "counts, classes = metric.get_metric()\n",
    "\n",
    "print(f\"No priming: {counts}\")\n",
    "\n",
    "context = [\"\"\"Answer the following question with either \"Easy to see\" or \"Hard to see\".\\n\"\"\",\n",
    "           \"Question: Is a child wearing blindfold easy to see, or hard to see?\", \"A: \\\" Easy to see . \\\"\",\n",
    "           \"Question: Is a doll wearing blindfold easy to see, or hard to see?\", \"A: \\\" Easy to see . \\\"\",\n",
    "           \"Question: Is a person wearing blindfold easy to see, or hard to see?\"]\n",
    "prompt_text = \"Answer: \"\n",
    "\n",
    "prompt = GPTPrompt(context, prompt_text)\n",
    "text = str(prompt)\n",
    "\n",
    "replicants = 20\n",
    "class_lookups = {\"easy\": [\"easy\"], \"hard\": [\"hard\", \"difficult\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "kwargs = {\"max_tokens\": 8, \"temperature\": 0.1}\n",
    "\n",
    "metric, responses = run_experiment(run_gpt_prompt, text, replicants, metric, kwargs)\n",
    "counts, classes = metric.get_metric()\n",
    "resp = run_gpt_prompt(str(prompt), None)\n",
    "counts, classes = metric.get_metric()\n",
    "\n",
    "print(f\"With priming: {counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does it hate Blindfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:13<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "context = [\"\"\"Answer the following question with either \"Easy to see\" or \"Hard to see\".\\n\"\"\",\n",
    "           \"Question: Are blind people easy to see, or hard to see?\"]\n",
    "prompt_text = \"Answer: \"\n",
    "\n",
    "prompt = GPTPrompt(context, prompt_text)\n",
    "replicants = 20\n",
    "class_lookups = {\"easy\": [\"easy\"], \"hard\": [\"hard\", \"difficult\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text = str(prompt)\n",
    "kwargs = {\"max_tokens\": 8, \"temperature\": 0.1}\n",
    "metric, responses = run_experiment(run_gpt_prompt, text, replicants, metric, kwargs)\n",
    "counts, classes = metric.get_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'easy': 0, 'hard': 0, 'other': 20}\n",
      "{'easy': [], 'hard': [], 'other': ['\\n Answer :   \\n \\n Answer', '\\n Answer  the  following  question  with  either', '__ \\n \\n Answer :   ________', '\\n Answer :   \\n \\n Answer', '\\n Answer  the  following  question  with  either', '\\n Answer  the  following  question  with  either', '_______ \\n \\n Answer  the  following  question', '\\n Answer  the  following  question  with  either', '\\n Answer :   \\n \\n Answer', '\\n Answer :   \\n \\n Answer', '\\n Answer  the  following  question  with  either', '__ \\n \\n Answer :   ________', '\\n Answer  the  following  question  with  either', '\\n Answer :   \\n \\n Answer', '\\n Answer :   \\n \\n Answer', '\\n Answer  the  following  question  with  either', '\\n Answer  the  following  question  with  either', '__ \\n \\n Answer :   ________', '\\n Answer  the  following  question  with  either', '\\n Answer  the  following  question  with  either']}\n"
     ]
    }
   ],
   "source": [
    "print(counts)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about \"promising\" vs \"asking\"\n",
    "\n",
    "Now it's time to look at other stimuli, like \"x promises y to do z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:06<00:00,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tom': 20, 'Mary': 0, 'other': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = [\"\"\"You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom promised Mary to call home.\\n\"\"\",\n",
    "           \"Question:  Who called home, Tom or Mary?\"]\n",
    "prompt_text = \"Answer: \"\n",
    "\n",
    "prompt = GPTPrompt(context, prompt_text)\n",
    "\n",
    "replicants = 20\n",
    "class_lookups = {\"Tom\": [\"tom\"], \"Mary\": [\"mary\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text = str(prompt)\n",
    "kwargs = {\"max_tokens\": 2, \"temperature\": 0.0}\n",
    "metric, responses = run_experiment(run_gpt_prompt, text, replicants, metric, kwargs)\n",
    "counts, classes = metric.get_metric()\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:07<00:00,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tom': 20, 'Mary': 0, 'other': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = [\"\"\"You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom asked Mary to call home.\\n\"\"\",\n",
    "           \"Question:  Who called home, Tom or Mary?\"]\n",
    "prompt_text = \"Answer: \"\n",
    "\n",
    "prompt = GPTPrompt(context, prompt_text)\n",
    "\n",
    "replicants = 20\n",
    "class_lookups = {\"Tom\": [\"tom\"], \"Mary\": [\"mary\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text = str(prompt)\n",
    "kwargs = {\"max_tokens\": 2, \"temperature\": 0.0}\n",
    "metric, responses = run_experiment(run_gpt_prompt, text, replicants, metric, kwargs)\n",
    "counts, classes = metric.get_metric()\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch the order of Tom and Mary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:07<00:00,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tom': 0, 'Mary': 14, 'other': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = [\"\"\"You will be given a context and a question. Answer the question with either \"Mary\" or \"Tom\".\\nContext: Mary promised Tom to call home.\\n\"\"\",\n",
    "           \"Question:  Who called home, Mary or Tom?\"]\n",
    "prompt_text = \"Answer: \"\n",
    "\n",
    "prompt = GPTPrompt(context, prompt_text)\n",
    "\n",
    "replicants = 20\n",
    "class_lookups = {\"Tom\": [\"tom\"], \"Mary\": [\"mary\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text = str(prompt)\n",
    "kwargs = {\"max_tokens\": 2, \"temperature\": 0.0}\n",
    "metric, responses = run_experiment(run_gpt_prompt, text, replicants, metric, kwargs)\n",
    "counts, classes = metric.get_metric()\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:07<00:00,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tom': 0, 'Mary': 20, 'other': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = [\"\"\"You will be given a context and a question. Answer the question with either \"Mary\" or \"Tom\".\\nContext: Mary asked Tom to call home.\\n\"\"\",\n",
    "           \"Question:  Who called home, Mary or Tom?\"]\n",
    "prompt_text = \"Answer: \"\n",
    "\n",
    "prompt = GPTPrompt(context, prompt_text)\n",
    "\n",
    "replicants = 20\n",
    "class_lookups = {\"Tom\": [\"tom\"], \"Mary\": [\"mary\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text = str(prompt)\n",
    "kwargs = {\"max_tokens\": 2, \"temperature\": 0.0}\n",
    "metric, responses = run_experiment(run_gpt_prompt, text, replicants, metric, kwargs)\n",
    "counts, classes = metric.get_metric()\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "For ask versus promise, the model gets \"promise\" right, but \"ask\" wrong. Maybe it's getting confused by asked, as in \"asked permission\" rather than \"asked for person 2 to do the thing\" \n",
    "\n",
    "## Switch ask to force\n",
    "force is higher factuality than ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tom': 0, 'Mary': 20, 'other': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:06<00:00,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tom': 18, 'Mary': 2, 'other': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = [\"\"\"You will be given a context and a question. Answer the question with either \"Mary\" or \"Tom\".\\nContext: Mary forced Tom to call home.\\n\"\"\",\n",
    "           \"Question:  Who called home, Mary or Tom?\"]\n",
    "prompt_text = \"Answer: \"\n",
    "\n",
    "prompt = GPTPrompt(context, prompt_text)\n",
    "\n",
    "replicants = 20\n",
    "class_lookups = {\"Tom\": [\"tom\"], \"Mary\": [\"mary\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text = str(prompt)\n",
    "kwargs = {\"max_tokens\": 2, \"temperature\": 0.0}\n",
    "metric, responses = run_experiment(run_gpt_prompt, text, replicants, metric, kwargs)\n",
    "counts, classes = metric.get_metric()\n",
    "\n",
    "print(counts)\n",
    "\n",
    "\n",
    "\n",
    "context = [\"\"\"You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom forced Mary to call home.\\n\"\"\",\n",
    "           \"Question:  Who called home, Tom or Mary?\"]\n",
    "prompt_text = \"Answer: \"\n",
    "\n",
    "prompt = GPTPrompt(context, prompt_text)\n",
    "\n",
    "replicants = 20\n",
    "class_lookups = {\"Tom\": [\"tom\"], \"Mary\": [\"mary\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text = str(prompt)\n",
    "kwargs = {\"max_tokens\": 2, \"temperature\": 0.0}\n",
    "metric, responses = run_experiment(run_gpt_prompt, text, replicants, metric, kwargs)\n",
    "counts, classes = metric.get_metric()\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Even with \"force\" it is conindexing with the subject rather than the object for object control \n",
    "\n",
    "## What about passives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:06<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tom': 0, 'Mary': 20, 'other': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:07<00:00,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tom': 20, 'Mary': 0, 'other': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = [\"\"\"You will be given a context and a question. Answer the question with either \"Mary\" or \"Tom\".\\nContext: Mary was forced by Tom to call home.\\n\"\"\",\n",
    "           \"Question:  Who called home, Mary or Tom?\"]\n",
    "prompt_text = \"Answer: \"\n",
    "\n",
    "prompt = GPTPrompt(context, prompt_text)\n",
    "\n",
    "replicants = 20\n",
    "class_lookups = {\"Tom\": [\"tom\"], \"Mary\": [\"mary\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text = str(prompt)\n",
    "kwargs = {\"max_tokens\": 2, \"temperature\": 0.0}\n",
    "metric, responses = run_experiment(run_gpt_prompt, text, replicants, metric, kwargs)\n",
    "counts, classes = metric.get_metric()\n",
    "\n",
    "print(counts)\n",
    "\n",
    "\n",
    "\n",
    "context = [\"\"\"You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom was forced by Mary to call home.\\n\"\"\",\n",
    "           \"Question:  Who called home, Tom or Mary?\"]\n",
    "prompt_text = \"Answer: \"\n",
    "\n",
    "prompt = GPTPrompt(context, prompt_text)\n",
    "\n",
    "replicants = 20\n",
    "class_lookups = {\"Tom\": [\"tom\"], \"Mary\": [\"mary\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text = str(prompt)\n",
    "kwargs = {\"max_tokens\": 2, \"temperature\": 0.0}\n",
    "metric, responses = run_experiment(run_gpt_prompt, text, replicants, metric, kwargs)\n",
    "counts, classes = metric.get_metric()\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "1. Doesn't have trouble with subject control in the case of \"easy to see\" and \"hard to see\" \n",
    "2. Unlike children, it has trouble with object control but not subject control \n",
    "    - MDP predicts that PRO will be co-indexed with the closest noun, e.g. that for \"Tom promised/asked Mary to call home\" it will be coindexed with Mary\n",
    "    - The model actually does the opposite of what children do -- coindexes with the subject (maximally far away) instead of the object. So it gets subject control (promise) correct but object control (ask) wrong. \n",
    "3. In following with Maratsos, model gets passives correct\n",
    "    - suggests that model is definitely not following the MDP \n",
    "\n",
    "## Follow-up question\n",
    "Maratsos (1974) hypothesizes that the semantic roles in a sentence determine control. \n",
    "\n",
    "1. Is the evidence so far consistent with that hypothesis\n",
    "    - easy vs hard stimuli: somewhat, in that it isn't getting them right all the time \n",
    "    - subject vs object control: could be consistent with a bias towards \"agent-subject of the matrix clause is the agent-subject of the embedded clause\" \n",
    "    - passives: **not** consistent with \"agent of the matrix clause is agent of the embedded clause. Here, matrix patient-subject is the agent in second clause. \n",
    "2. How can we manipulate semantic roles here to get evidence on whether the model is using semantic role information \n",
    "    - right now we have \n",
    "\n",
    "\n",
    "    | matrix subj. | embedded subj. | experiment  | \n",
    "    | ------------ | ---------- | ----------- | \n",
    "    | agent | agent      | promise     | \n",
    "    | agent | patient | ask | \n",
    "    | patient | patient | passive told/asked | \n",
    "\n",
    "    - can we get patient agent? Something like \"Tom was promised by Mary to call home\" but grammatical \n",
    "\n",
    "## What about propose\n",
    "propose is ambiguous between subject and object control\n",
    "e.g. \n",
    "- \"Tom proposed to Mary to leave\" --> they both leave\n",
    "- \"Tom proposed to Bill to be his editor\" --> either Bill or Tom is the editor\n",
    "- \"Tom proposed to Mary to be her editor\" --> Tom is the editor (coerced by \"her\")\n",
    "- \"Tom proposed to Mary to be his editor\" --> Mary is the editor (coerced by \"his\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:08<00:00,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tom': 0, 'Mary': 0, 'both': 0, 'other': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = [\"\"\"You will be given a context and a question. Answer the question with \"Tom\", \"Mary\", or \"both\".\\nContext: Tom proposed to Mary to leave.\\n\"\"\",\n",
    "           \"Question: Who will leave, Tom, Mary, or both?\"]\n",
    "prompt_text = \"Answer: \"\n",
    "\n",
    "prompt = Prompt(context, prompt_text)\n",
    "\n",
    "replicants = 20\n",
    "class_lookups = {\"Tom\": [\"tom\"], \"Mary\": [\"mary\"], \"both\": [\"both\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text = str(prompt)\n",
    "kwargs = {\"max_tokens\": 2, \"temperature\": 0.0}\n",
    "metric, responses = run_experiment(text, replicants, metric, kwargs)\n",
    "counts, classes = metric.get_metric()\n",
    "\n",
    "print(counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:07<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tom': 0, 'Bill': 0, 'other': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = [\"\"\"You will be given a context and a question. Answer the following yes-no question.\\nContext: Tom suggested Bill to be his editor.\\n\"\"\",\n",
    "           \"Question: Will Tom be the editor?\"]\n",
    "\n",
    "prompt_text = \"Answer: \"\n",
    "\n",
    "prompt = Prompt(context, prompt_text)\n",
    "\n",
    "replicants = 20\n",
    "class_lookups = {\"Tom\": [\"tom\", \"yes\"], \"Bill\": [\"bill\", \"no\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text = str(prompt)\n",
    "kwargs = {\"max_tokens\": 2, \"temperature\": 0.0}\n",
    "metric, responses = run_experiment(text, replicants, metric, kwargs)\n",
    "counts, classes = metric.get_metric()\n",
    "\n",
    "print(counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:07<00:00,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tom': 1, 'Mary': 4, 'other': 15}\n",
      "[('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', 'Mary'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', '\\n'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', '\\n'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', '\"'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', '\\n'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', '______'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', '\\n'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', '\\n'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', '\\n'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', '\\n'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', 'As'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', '\\n'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', 'Mary'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', 'Tom'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', '\\n'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', 'Mary'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', '\\n'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', 'Mary'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', '\\n'), ('You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\\nQuestion: Who will be an editor, Tom or Mary?\\nAnswer: ', '\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "context = [\"\"\"You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested to Mary to be her editor.\\n\"\"\",\n",
    "           \"Question: Who will be an editor, Tom or Mary?\"]\n",
    "prompt_text = \"Answer: \"\n",
    "\n",
    "prompt = GPTPrompt(context, prompt_text)\n",
    "\n",
    "replicants = 20\n",
    "class_lookups = {\"Tom\": [\"tom\"], \"Mary\": [\"mary\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text = str(prompt)\n",
    "kwargs = {\"max_tokens\": 2, \"temperature\": 0.5}\n",
    "metric, responses = run_experiment(run_gpt_prompt, text, replicants, metric, kwargs)\n",
    "counts, classes = metric.get_metric()\n",
    "\n",
    "print(counts)\n",
    "print(responses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:09<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tom': 6, 'Mary': 14, 'other': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = [\"\"\"You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom suggested Mary to be his editor.\\n\"\"\",\n",
    "           \"Question:  Who will be an editor, Tom or Mary?\"]\n",
    "prompt_text = \"Answer: \"\n",
    "\n",
    "prompt = GPTPrompt(context, prompt_text)\n",
    "\n",
    "replicants = 20\n",
    "class_lookups = {\"Tom\": [\"tom\"], \"Mary\": [\"mary\"]}\n",
    "metric = StringMetric(class_lookups)\n",
    "text = str(prompt)\n",
    "kwargs = {\"max_tokens\": 2, \"temperature\": 0.0}\n",
    "metric, responses = run_experiment(run_gpt_prompt, text, replicants, metric, kwargs)\n",
    "counts, classes = metric.get_metric()\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does GPT3 hate object control? \n",
    "\n",
    "Let's test a bunch of different object control verbs:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:07<00:00,  2.56it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.62it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.34it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.60it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.56it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.20it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.39it/s]\n",
      "100%|██████████| 20/20 [00:06<00:00,  2.91it/s]\n",
      "100%|██████████| 20/20 [00:06<00:00,  2.92it/s]\n",
      "100%|██████████| 20/20 [00:06<00:00,  2.87it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.71it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.68it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.79it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.41it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.73it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.72it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.68it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.59it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.46it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.29it/s]\n",
      "100%|██████████| 20/20 [00:06<00:00,  2.88it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.50it/s]\n",
      "100%|██████████| 20/20 [00:06<00:00,  2.88it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.47it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.45it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.48it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.58it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.43it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.60it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.47it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.85it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.37it/s]\n",
      "100%|██████████| 20/20 [00:06<00:00,  2.88it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.32it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.61it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.25it/s]\n",
      "100%|██████████| 20/20 [00:06<00:00,  2.91it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.74it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.42it/s]\n",
      "100%|██████████| 20/20 [00:06<00:00,  2.92it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tom': 214, 'Mary': 602, 'other': 24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "object_control_verbs = [\"told\", \"ordered\", \"exhorted\", \"begged\", \"entreated\", \"called upon\", \"reminded\", \"urged\", \"asked\", \"pleaded with\", \"persuaded\", \"convinced\", \"forced\", \"pushed\"]\n",
    "\n",
    "actions = [(\"to leave\", \"left\"), (\"to call home\", \"called home\"), (\"to reply\", \"replied\")]\n",
    "\n",
    "responses_by_verb = {}\n",
    "\n",
    "metric = StringMetric(class_lookups)\n",
    "class_lookups = {\"Tom\": [\"tom\"], \"Mary\": [\"mary\"]}\n",
    "for verb in object_control_verbs:\n",
    "    for infinitive, past in actions:\n",
    "        context = [f\"\"\"You will be given a context and a question. Answer the question with either \"Tom\" or \"Mary\".\\nContext: Tom {verb} Mary {infinitive}.\\n\"\"\",\n",
    "                f\"Question:  Who {past}, Tom or Mary?\"]\n",
    "        prompt_text = \"Answer: \"\n",
    "\n",
    "        key = f\"{verb}, {infinitive}\"\n",
    "\n",
    "        prompt = GPTPrompt(context, prompt_text)\n",
    "\n",
    "        replicants = 20\n",
    "        text = str(prompt)\n",
    "        kwargs = {\"max_tokens\": 2, \"temperature\": 0.0}\n",
    "        metric, responses = run_experiment(run_gpt_prompt, text, replicants, metric, kwargs)\n",
    "        responses_by_verb[key] = responses \n",
    "\n",
    "counts, classes = metric.get_metric()\n",
    "print(counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "told, to call home: Tom: 11, Mary: 9\n",
      "ordered, to call home: Tom: 9, Mary: 11\n",
      "exhorted, to call home: Tom: 2, Mary: 18\n",
      "begged, to leave: Tom: 6, Mary: 14\n",
      "begged, to call home: Tom: 18, Mary: 2\n",
      "entreated, to call home: Tom: 8, Mary: 12\n",
      "called upon, to leave: Tom: 3, Mary: 17\n",
      "called upon, to call home: Tom: 20, Mary: 0\n",
      "reminded, to call home: Tom: 15, Mary: 5\n",
      "urged, to call home: Tom: 6, Mary: 14\n",
      "asked, to call home: Tom: 17, Mary: 3\n",
      "asked, to reply: Tom: 2, Mary: 14\n",
      "pleaded with, to call home: Tom: 5, Mary: 15\n",
      "persuaded, to leave: Tom: 10, Mary: 10\n",
      "persuaded, to call home: Tom: 20, Mary: 0\n",
      "persuaded, to reply: Tom: 1, Mary: 19\n",
      "convinced, to leave: Tom: 5, Mary: 15\n",
      "convinced, to call home: Tom: 18, Mary: 2\n",
      "forced, to leave: Tom: 10, Mary: 10\n",
      "forced, to call home: Tom: 16, Mary: 4\n",
      "pushed, to call home: Tom: 12, Mary: 8\n"
     ]
    }
   ],
   "source": [
    "for key, responses in responses_by_verb.items():\n",
    "    mary = 0\n",
    "    tom = 0\n",
    "    other = 0\n",
    "    for __, r in responses:\n",
    "        if r.strip().lower() == \"mary\": \n",
    "            mary += 1\n",
    "        elif r.strip().lower() == \"tom\": \n",
    "            tom += 1\n",
    "        else:\n",
    "            other += 1\n",
    "    if tom > 0:\n",
    "        print(f\"{key}: Tom: {tom}, Mary: {mary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec6008f546f3cd66431ed7a4f2e2b63949ea9a2fd324a00b9368fd8a5a5333f5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('openai': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
