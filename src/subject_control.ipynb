{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from experiment import Experiment\n",
    "from api_tools import (FixedGPTPrompt, \n",
    "                       FixedPassiveGPTPrompt, \n",
    "                       FixedT5Prompt, \n",
    "                       FixedPassiveT5Prompt, \n",
    "                       run_ai21_prompt, \n",
    "                       run_gpt_prompt, \n",
    "                       run_t5_prompt)\n",
    "\n",
    "from metrics import accuracy_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject Control\n",
    "Starting with the **subject** control experiments, we will look at how 4 LMs do:\n",
    "1. GPT3\n",
    "2. T5 for QA \n",
    "3. Jurassic large \n",
    "4. Jurassic Jumbo\n",
    "\n",
    "## Experimental settings\n",
    "We're looking at different name pairs to control for gender. We have 2 male-female name pairs, 1 male-male pair, 1 female-female pair, and 1 neutral-neutral. \n",
    "We're still looking at 5 different actions, but only one verb: promise. \n",
    "\n",
    "We can later coerce (with gender) subject control on \"suggested\" and \"proposed\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = [(\"Tom\", \"Mary\"), (\"Tom\", \"Bill\"), (\"Mary\", \"Bill\"), (\"Ellen\", \"Mary\"), (\"Morgan\", \"Jaime\"), (\"Iago\", \"Hamlet\"), (\"Jules\", \"Yves\"), (\"Kurt\", \"Lena\")]\n",
    "names = [(\"Tom\", \"Mary\"), (\"Tom\", \"Bill\"), (\"Mary\", \"Bill\"), (\"Ellen\", \"Mary\"), (\"Morgan\", \"Jaime\")]\n",
    "# verbs = [\"told\", \"ordered\", \"called upon\", \"reminded\", \"urged\", \"asked\", \"persuaded\", \"convinced\", \"forced\", \"pushed\"]\n",
    "verbs = [\"promised\"]\n",
    "actions = [(\"to leave\", \"left\"), (\"to call home\", \"called home\"), (\"to reply\", \"replied\"), (\"to wipe the counter\", \"wiped the counter\"), (\"to dance\", \"danced\")]\n",
    "correct_index = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 3\n",
    "For GPT3, inference is not deterministic, so we're running 5 replicants per prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gpt_kwargs = {\"max_tokens\": 2, \"temperature\": 0.0}\n",
    "# gpt_subject_control_experiment  = Experiment(\"gpt3\", \"subject-control\", FixedGPTPrompt, run_gpt_prompt, 5, gpt_kwargs)\n",
    "\n",
    "# gpt_subject_control_experiment.run(names, correct_index, verbs, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt_df = gpt_subject_control_experiment.format_results()\n",
    "\n",
    "# gpt_df.to_csv(\"/Users/Elias/child-lm/results/gpt_subject_control_swap_names.csv\")\n",
    "\n",
    "gpt_df = pd.read_csv(\"/Users/Elias/child-lm/results/gpt_subject_control_swap_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = gpt_df[gpt_df['true'] == gpt_df['pred']]\n",
    "incorrect = gpt_df[gpt_df['true'] != gpt_df['pred']]\n",
    "\n",
    "print(f\"accuraccy: {(len(correct)/ len(gpt_df)) * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 for QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# t5_subject_control_experiment  = Experiment(\"t5\", \"subject-control\", FixedT5Prompt, run_t5_prompt, 1, None)\n",
    "\n",
    "# t5_subject_control_experiment.run(names, correct_index, verbs, actions, do_swap = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_df = t5_subject_control_experiment.format_results()\n",
    "\n",
    "# t5_df.to_csv(\"/Users/Elias/child-lm/results/t5_subject_control.csv\")\n",
    "\n",
    "t5_df = pd.read_csv(\"/Users/Elias/child-lm/results/t5_subject_control.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = t5_df[t5_df['true'] == t5_df['pred']]\n",
    "incorrect = t5_df[t5_df['true'] != t5_df['pred']]\n",
    "\n",
    "print(f\"accuraccy: {(len(correct)/ len(t5_df)) * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jurassic Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# jurassic_kwargs = {\"maxTokens\": 2, \"temperature\": 0.0}\n",
    "# jurassic_subject_control_experiment  = Experiment(\"jurassic-large\", \"subject-control\", FixedGPTPrompt, run_ai21_prompt, 1, jurassic_kwargs)\n",
    "\n",
    "# jurassic_subject_control_experiment.run(names, correct_index, verbs, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jurassic_df = jurassic_subject_control_experiment.format_results()\n",
    "\n",
    "# jurassic_df.to_csv(\"/Users/Elias/child-lm/results/jurassic_subject_control_swap_names.csv\")\n",
    "\n",
    "jurassic_df = pd.read_csv(\"/Users/Elias/child-lm/results/jurassic_subject_control_swap_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = jurassic_df[jurassic_df['true'] == jurassic_df['pred']]\n",
    "incorrect = jurassic_df[jurassic_df['true'] != jurassic_df['pred']]\n",
    "\n",
    "print(f\"accuraccy: {(len(correct)/ len(jurassic_df)) * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Coerced examples with gender\n",
    "\n",
    "By using gendered names and pronouns, we can coerce subject or object control from \"suggested\", \"offered\", and \"proposed\", e.g. \n",
    "\n",
    "- Mary proposed to Tom to be his editor\n",
    "- Tom suggested to Mary to be her editor \n",
    "- Mary offered to Tom to be his editor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = [\"promised\", \"offered\", \"suggested\", \"proposed\"]\n",
    "his_names = [(\"Tom\", \"Mary\"), (\"Bill\", \"Mary\"), (\"James\", \"Mary\"), (\"Tom\", \"Sally\"), (\"Bill\", \"Sally\"), (\"James\", \"Sally\")]\n",
    "actions = [(\"to be her editor\", \"was the editor\")]\n",
    "correct_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gpt_kwargs = {\"max_tokens\": 2, \"temperature\": 0.0}\n",
    "# gendered_gpt_subject_control_experiment  = Experiment(\"gpt3\", \"subject-control\", FixedGPTPrompt, run_gpt_prompt, 5, gpt_kwargs)\n",
    "# gendered_gpt_subject_control_experiment.run(his_names, correct_index, verbs, actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gendered_gpt_df = gendered_gpt_subject_control_experiment.format_results()\n",
    "\n",
    "# gendered_gpt_df.to_csv(\"/Users/Elias/child-lm/results/gpt_gendered_subject_control_swap_names.csv\")\n",
    "gendered_gpt_df = pd.read_csv(\"/Users/Elias/child-lm/results/gpt_gendered_subject_control_swap_names.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = gendered_gpt_df[gendered_gpt_df['true'] == gendered_gpt_df['pred']]\n",
    "# incorrect = gpt_df[gpt_df['true'] != gpt_df['pred']]\n",
    "\n",
    "print(f\"accuraccy: {(len(correct)/ len(gendered_gpt_df)) * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 for QA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gendered_t5_subject_control_experiment  = Experiment(\"t5\", \"subject-control\", FixedT5Prompt, run_t5_prompt, 1, None)\n",
    "gendered_t5_subject_control_experiment.run(his_names, correct_index, verbs, actions)\n",
    "gendered_t5_df = gendered_t5_subject_control_experiment.format_results()\n",
    "gendered_t5_df.to_csv(\"/Users/Elias/child-lm/results/t5_gendered_subject_control_swap_names.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = gendered_t5_df[gendered_t5_df['true'] == gendered_t5_df['pred']]\n",
    "# incorrect = gpt_df[gpt_df['true'] != gpt_df['pred']]\n",
    "\n",
    "print(f\"accuraccy: {(len(correct)/ len(gendered_t5_df)) * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jurassic Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jurassic_kwargs = {\"maxTokens\": 2, \"temperature\": 0.0}\n",
    "# gendered_jurassic_subject_control_experiment  = Experiment(\"jurassic\", \"subject-control\", FixedGPTPrompt, run_ai21_prompt, 1, jurassic_kwargs)\n",
    "# gendered_jurassic_subject_control_experiment.run(his_names, correct_index, verbs, actions)\n",
    "# gendered_jurassic_df = gendered_jurassic_subject_control_experiment.format_results()\n",
    "# gendered_jurassic_df.to_csv(\"/Users/Elias/child-lm/results/jurassic_gendered_subject_control_swap_names.csv\")\n",
    "gendered_jurassic_df = pd.read_csv(\"/Users/Elias/child-lm/results/jurassic_gendered_subject_control_swap_names.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = gendered_jurassic_df[gendered_jurassic_df['true'] == gendered_jurassic_df['pred']]\n",
    "# incorrect = gpt_df[gpt_df['true'] != gpt_df['pred']]\n",
    "\n",
    "print(f\"accuraccy: {(len(correct)/ len(gendered_jurassic_df)) * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import accuracy_report\n",
    "\n",
    "accuracy_report(gendered_jurassic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passives \n",
    "\n",
    "Do passives here make sense? To me \n",
    "- Mary was promised by Tom to leave\n",
    "Does not sound acceptable, or if it is accepetable, Mary is the one leaving, unlike \"Tom promised Mary to leave\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec6008f546f3cd66431ed7a4f2e2b63949ea9a2fd324a00b9368fd8a5a5333f5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('openai': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
